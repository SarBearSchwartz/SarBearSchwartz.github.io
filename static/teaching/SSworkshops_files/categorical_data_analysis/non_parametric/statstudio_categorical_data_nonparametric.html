<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Categorical Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Sarah Schwartz" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="pres2.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Categorical Data Analysis
## Randomization Tests
### Dr. Sarah Schwartz
### Fall 2021

---




background-image: url(figures/StatStudio_Workshop_categorical_icon.png)
background-position: 50% 50%
background-size: 1200px


---

background-image: url(figures/fig_scale_variable.png)
background-position: 50% 80%
background-size: 600px

# Scale vs. Variable


---
# Scale vs. Variable

.pull-left[
## MEASUREMENT SCALE

.large[
- .coral[Nominal] = named groupings, no meaningful order
- .dcoral[Ordinal] = groupings that do have natural order
- .nicegreen[Interval] = precise units that are equally spaced
- .bluer[Ratio]	= interval + true zero point
]]

.pull-right[
## VARIABLE TYPE

.large[
- .coral[Categorical or Discrete] = finite, countable number of levels, no intermediate values possible	
- .nicegreen[Numeric or Continuous] = infinite intermediate values are possible, at least in theory
]

*NOTE: due to limits on measurement precision, observed data may be discrete, even though the underlying construct is continuous*
]


---

## Ordinal, Interval, Ratio

- .dcoral[Ordinal] = groupings that do have natural order
- .nicegreen[Interval] = precise units that are equally spaced
- .bluer[Ratio]	= interval + true zero point

.pull-left[

&lt;img src="figures/textbook_fig_1.1.PNG" width="1205" style="display: block; margin: auto;" /&gt;

]


.pull-right[

&lt;img src="figures/textbook_fig_1.2.PNG" width="1161" style="display: block; margin: auto;" /&gt;

]


---

## Discrete vs. Continuous

- .coral[Categorical or Discrete] = finite, countable number of levels, no intermediate values possible	
- .nicegreen[Numeric or Continuous] = infinite intermediate values are possible, at least in theory


.pull-left[
&lt;img src="figures/textbook_fig_1.3.PNG" width="1369" style="display: block; margin: auto;" /&gt;
]

*NOTE: due to limits on measurement precision, observed data may be discrete, even though the underlying construct is continuous*

---

## SCORE EXAMPLES


.pull-left[

### Temperature

* **Nominal**
   + Comfortable vs Uncomfortable
   
* **Ordinal**
   + Ice, Chilly, Comfy, Warm, Hates

* **Interval**
   + Celsius (C)
   + Fahrenheit (F) 

* **Ratio**
   + Kelvin (K)
]


--

.pull-right[

### Depression

* **Nominal**
   + Early onset, Late onset, Chronic
   
* **Ordinal**
   + None, mild, moderate, severe

* **Interval-Ratio'ish**
   + Beck Depression Inventory, average of 21 Likert items each on a scale of 0-3

]

---

# STATISTICAL INFERENCE

### Hypothesis

* Null: no relationship or no difference
* Alternative: some relationship or a non-zero difference

### Assumptions

* independence of observations
* normality
* homogeneity of variance

### Test Statistic

* assuming the null hypothesis is true
* measure of the extremeness of our data

### p-values

&gt; The probability of observing data as extreme or more extreme than what we observed, given the null hypothesis is true.


---

# TYPES of STATISTICAL INFERENCE

.pull-left[

## Parametric

* Based on a specific distribution (i.e. normal)
* Interval, ratio scales
* *can be* more powerful
* accuracy influenced by outliers

Ex) t-test, ANOVA, regression

]


--

.pull-right[

## Non-Parametric

* oftern used for ordinal or norminal scores
* NOT base on any specific distribution
* no assumption makes it more flexible

Ex) Exact tests, Mann-Whitney W, Rank tests

]

---

## Parametric vs. Non-Parametric - Situation A

**Type of Analysis**

Compare means between two distinct/independent groups

**Example Research Question**

Is the mean systolic blood pressure (at baseline) for patients assigned to placebo different from the mean for patients assigned to the treatment group?


**Parametric Method**

--

* Two-sample t-test 


.dcoral[**Non-parametric Method**]

--

.coral[
* Wilcoxon rank-sum test ]




---

## Parametric vs. Non-Parametric - Situation B

**Type of Analysis**

Compare two quantitative measurements taken from the same individual

**Example Research Question**

Was there a significant change in systolic blood pressure between baseline and the six-month followup measurement in the treatment group?

**Parametric Method**

--

* Paired t-test

.dcoral[**Non-parametric Method**]

--

.coral[
* Wilcoxon signed-rank test ]




---

## Parametric vs. Non-Parametric - Situation C


**Type of Analysis**

Compare means between three or more distinct/independent groups


**Example Research Question**

If our experiment had three groups (e.g., placebo, new drug #1, new drug #2), we might want to know whether the mean systolic blood pressure at baseline differed  among the three groups?


**Parametric Method**

--

* Analysis of variance (ANOVA)

.dcoral[**Non-parametric Method**]

--

.coral[
* Kruskal-Wallis test]




---

## Parametric vs. Non-Parametric - Situation D

**Type of Analysis**

Estimate the degree of association between two quantitative variables

**Example Research Question**

Is systolic blood pressure associated with the patient’s age?


**Parametric Method**

--

* Pearson coefficient of correlation

.dcoral[**Non-parametric Method**]

--

.coral[
* Spearman’s rank correlation]


---

# Randomization Test

NOT Parametric:

Assume a population's distribution and therefor the sampling distribution follows a specific distribution (like the normal curve)

IS Non-Parametric:

Determine the sampling distribution (called the “permutation distribution”) by **resampling the observed data**.



Specifically, we can “shuffle” or permute the observed data 

&gt; by assigning different outcome values to each observation from among the set of actually observed outcomes

---

&lt;!-- Ed Boone: The Randomization Test (Using R)  (12.5 min)--&gt;

&lt;iframe width="1000" height="750" src="https://www.youtube.com/embed/_a4EwbRK9cs?controls=0&amp;amp;start=11" frameborder="10" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;






---

## The general premise of randomization methods 

&gt; "To summarize, Ronald Fisher ﬁrst developed randomization methods in the mid-1930s. The general premise of randomization methods is that, under the `\(H_0\)`, scores from different groups should be **interchangeable**. Thus, researchers can determine the probability of obtaining a statistic from a given set of data by:
&gt;
&gt; (a) repeatedly rearranging data between groups, 
&gt;
&gt; (b) calculating the statistic of interest for each set of rearranged data (e.g., a mean difference or t-value), 
&gt;
&gt; (c) determining how many of those statistics are as extreme or more extreme than the statistic obtained from an experiment, and 
&gt;
&gt; (d) dividing that number of extreme statistics by the total number of statistics obtained by rearranging the data. 
&gt;
&gt; Following this basic logic, randomization-based alternatives have been developed for many of the statistical analyses that are common in the behavioral sciences."
&gt;
&gt; - [Randomization tests as alternative analysis methods for behavior-analytic data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6524641/)


---

## Types of Randomization Tests

### Exact Methods

* Consider ALL possible rearrangements
* "Complete enumeration" of options
* "Fisher's Exact Test" for 2x2 contingency table


.pull-left[
### Permutations Methods

* Sample with-OUT replacement

* Best for hypothesis tests (exchangability)
]

.pull-right[
### Bootstrapping Methods

* Sample WITH replacement

* Best for confidence intervals (spread)
]



---

# Tests of Independence

## Hypotheses

Independence tests are used to determine if there is a significant **relationship between two categorical variables**. 

--
.bluer[
$$
H_0: \text{the variables are independent} 
$$

There is no relationship between the two categorical variables. Knowing the value of one variable does not help to predict the value of the other variable.
]

--

.nicegreen[
$$
H_1: \text{the variables are dependent}
$$

There is a relationship between the two categorical variables. Knowing the value of one variable helps to predict the value of the other variable.
]

---

## Tests of Independence

&gt; Based on contingency table (cross tabulations) for 2 nominal variables 

### Chi-squared Test *(Traditional, Parametric)*

Used when the .nicegreen[sample is large enough]. The *p*-value is an **approximation** that becomes exact when the **sample becomes infinite**, which is the case for many statistical tests

--

### Fisher's Exact Test *(Randomized, Non-Parametric)*

Used when the .nicegreen[sample is small]. The *p*-value is **EXACT** and is not an approximation

--

### Selecting between the 2

Chi-square test is not appropriate when the .bluer[**expected values in one of the cells of the contingency table is less than 5**], and in this case the Fisher’s exact test is preferred 


---

&lt;!-- ChangSchool: Lady Tasting Tea - Inferential Statistics and Experimental Design  (3 min)--&gt;

&lt;iframe width="1000" height="750" src="https://www.youtube.com/embed/lgs7d5saFFc?controls=0&amp;amp;start=11" frameborder="10" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;




---

## Example 1: 2 nominal variables

.pull-left[


```r
library(tidyverse)

df_1 &lt;- expand.grid(gender = c("Male", "Female"),
                    drug   = c("A", "B")) %&gt;% 
  purrr::map_df(., rep, c(2, 7, 4, 9)) %&gt;% 
  tibble::rownames_to_column(var = "id") %&gt;% 
  dplyr::mutate_all(factor)
```


```r
head(df_1)
```

```
# A tibble: 6 x 3
  id    gender drug 
  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;
1 1     Male   A    
2 2     Male   A    
3 3     Female A    
4 4     Female A    
5 5     Female A    
6 6     Female A    
```


]


.pull-right[

```r
df_1 %&gt;% 
  furniture::table1(drug,
                    splitby = ~ gender,
                    total = TRUE,
                    digits = 2,
                    output = "markdown")
```



|      |   Total    |   Male    |  Female   |
|------|------------|-----------|-----------|
|      |   n = 22   |   n = 6   |  n = 16   |
| drug |            |           |           |
|  A   | 9 (40.9%)  | 2 (33.3%) | 7 (43.8%) |
|  B   | 13 (59.1%) | 4 (66.7%) | 9 (56.2%) |

]


---

### Chi-squared Test of Independence

.pull-leftbig[

The data must be tabulated with the `table()` function prior to running the `\(\chi^2\)` Test of Independenct.  The option of `correct = FALSE` indications to NOT apply Yates' continuity correction.


```r
fit_chisq_1 &lt;- df_1 %&gt;%
  dplyr::select(-id) %&gt;% 
  table() %&gt;% 
  chisq.test(correct = FALSE)

fit_chisq_1
```

```

	Pearson's Chi-squared test

data:  .
X-squared = 0.19587, df = 1, p-value = 0.6581
```
]


--

.pull-rightsmall[

Expected cell counts may be decimals


```r
fit_chisq_1$expected
```

```
        drug
gender          A        B
  Male   2.454545 3.545455
  Female 6.545455 9.454545
```

Observed cell counts are always whole numbers


```r
fit_chisq_1$observed
```

```
        drug
gender   A B
  Male   2 4
  Female 7 9
```

]


---

### Fisher's Exact Test of Independence



```r
df_1 %&gt;%
  dplyr::select(-id) %&gt;% 
  table() %&gt;% 
  fisher.test()
```

```

	Fisher's Exact Test for Count Data

data:  .
p-value = 1
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
 0.04634087 6.28626517
sample estimates:
odds ratio 
 0.6556733 
```

--

Compared to the Chi-squared p-value:


```r
fit_chisq_1$p.value
```

```
[1] 0.6580762
```





---

## Example 2: 2 nominal variables


.pull-left[

```r
df_2 &lt;- expand.grid(abuse   = c("Yes", "No"),
                    violent = c("Yes", "No")) %&gt;% 
  purrr::map_df(., rep, c(16, 8, 6, 14)) %&gt;% 
  tibble::rownames_to_column(var = "id") %&gt;% 
  dplyr::mutate_all(factor)
```



```r
head(df_2)
```

```
# A tibble: 6 x 3
  id    abuse violent
  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  
1 1     Yes   Yes    
2 2     Yes   Yes    
3 3     Yes   Yes    
4 4     Yes   Yes    
5 5     Yes   Yes    
6 6     Yes   Yes    
```
]


.pull-right[

```r
df_2 %&gt;% 
  furniture::table1(violent,
                    splitby = ~ abuse,
                    total = TRUE,
                    digits = 2,
                    output = "markdown")
```



|         |   Total    |    Yes     |     No     |
|---------|------------|------------|------------|
|         |   n = 44   |   n = 22   |   n = 22   |
| violent |            |            |            |
|   Yes   | 24 (54.5%) | 16 (72.7%) | 8 (36.4%)  |
|   No    | 20 (45.5%) | 6 (27.3%)  | 14 (63.6%) |
]




---

### Chi-squared Test of Independence

.pull-leftbig[


```r
fit_chisq_2 &lt;- df_2 %&gt;%
  dplyr::select(-id) %&gt;% 
  table() %&gt;% 
  chisq.test(correct = FALSE)

fit_chisq_2
```

```

	Pearson's Chi-squared test

data:  .
X-squared = 5.8667, df = 1, p-value = 0.01543
```

]

--

.pull-rightsmall[


```r
fit_chisq_2$expected
```

```
     violent
abuse Yes No
  Yes  12 10
  No   12 10
```


```r
fit_chisq_2$observed
```

```
     violent
abuse Yes No
  Yes  16  6
  No    8 14
```

]

---

### Fisher's Exact Test of Independence



```r
df_2 %&gt;%
  dplyr::select(-id) %&gt;% 
  table() %&gt;% 
  fisher.test()
```

```

	Fisher's Exact Test for Count Data

data:  .
p-value = 0.03286
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  1.108746 20.515528
sample estimates:
odds ratio 
  4.490598 
```


--

Compared to the Chi-squared p-value:


```r
fit_chisq_2$p.value
```

```
[1] 0.01543024
```



---

&lt;!-- Shonda Kuiper: C1b: Introduction to Randomization Tests  (5.5 min)--&gt;

&lt;iframe width="1000" height="750" src="https://www.youtube.com/embed/3zIaY2EwzN4?controls=0&amp;amp;start=0" frameborder=10" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;





---

&lt;!-- apethan: Confidence Intervals: Intro to bootstrapping proportions (11 min)--&gt;

&lt;iframe width="1000" height="750" src="https://www.youtube.com/embed/655X9eZGxls?controls=0&amp;amp;start=0" frameborder=10" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;






---

## Example 3: IV = continuous, DV = categorical

.pull-leftbig[
Darwin planted **15 pairs** of corn plants, one of which was produced through **cross pollination**, and the other of which was produced through **self-pollination**. 

Within each pair of plants, Darwin controlled variables such as type of soil, time of planting, and other growing conditions. 


```r
library(PairedData)

data(Corn, package = "PairedData")

Corn_long &lt;- Corn %&gt;% 
  dplyr::select(-pot) %&gt;% 
  tidyr::pivot_longer(cols = c(Crossed, Self),
                      names_to = "group",
                      values_to = "yield") %&gt;% 
  dplyr::mutate(group = factor(group))
```

]

.pull-rightsmall[


```r
head(Corn_long, n = 12)
```

```
# A tibble: 12 x 3
    pair group   yield
   &lt;int&gt; &lt;chr&gt;   &lt;int&gt;
 1     1 Crossed   188
 2     1 Self      130
 3     2 Crossed    96
 4     2 Self      163
 5     3 Crossed   168
 6     3 Self      160
 7     4 Crossed   176
 8     4 Self      160
 9     5 Crossed   153
10     5 Self      147
11     6 Crossed   172
12     6 Self      149
```

]


---

.pull-left[

Descriptive Summary Statistics


```r
Corn_long %&gt;% 
  furniture::table1(yield,
                    splitby = ~ group,
                    digits = 2,
                    output = "markdown")
```



|       |    Crossed     |      Self      |
|-------|----------------|----------------|
|       |     n = 15     |     n = 15     |
| yield |                |                |
|       | 161.53 (28.94) | 140.00 (16.64) |

]

.pull-right[

Standard, parametric Paired t-Test


```r
t.test(yield ~ group,
       data = Corn_long,
       paired = TRUE,
       alternative = "greater")
```

```

	Paired t-test

data:  yield by group
t = 2.1781, df = 14, p-value = 0.0235
alternative hypothesis: true difference in means is greater than 0
95 percent confidence interval:
 4.120256      Inf
sample estimates:
mean of the differences 
               21.53333 
```


]

---

Fisher reasoned that **randomly switching** pairs of plants between groups would not alter the expected value for the mean of each group under the H_0.

So he calculated **every possible rearrangement** of plants between groups while keeping pairs of plants matched.

He retained the matched pairs because Darwin controlled growing factors within pairs but these factors varied between pairs. 

For each of these rearrangements, Fisher (1935) **calculated absolute differences** in the heights of plants between groups by summing the heights of all plants in each group and then subtracting this sum for the self-pollinated group from that of the cross-pollinated group. 

Fisher then determined the probability of obtaining a between-group height difference that was as or more extreme than the actual difference that Darwin (1876) obtained (i.e., a difference of 39.25 inches between groups, in favor of cross-pollinated.

---

.pull-leftsmall[
LEFT: EMPIRICAL = Relative-frequency histogram showing mean differences from rearrangements of Darwin’s (1876) experiment on the heights of corn plants. The dashed lines labeled “D = − 2.62” and “D = 2.62” represent the mean difference in height between plants that received different pollination methods that Darwin (1876) obtained. 

RIGHT: THEORETICAL = Probability density for the t distribution with 14 degrees of freedom. The dashed lines labeled “t = −2.15” and “t = 2.15” represent the two-sided t-values derived from Darwin’s data.
]

.pull-rightbig[

&lt;img src="figures/fisher_corn_exact.jpg" width="95%" style="display: block; margin: auto;" /&gt;

]


---

[Online Resource](https://bookdown.org/curleyjp0/psy317l_guides5/permutation-testing.html#permutation-test-for-a-paired-t-test)

.pull-left[


```r
Corn_long %&gt;% 
  ggplot(aes(x = group, 
             y = yield, 
             fill = group)) +
  geom_boxplot(alpha = .5, 
               outlier.shape = NA) +
  geom_jitter(width = .1,
              size = 2) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "Type of Pollination",
       y = "Yield")
```

]

.pull-right[
&lt;img src="statstudio_categorical_data_nonparametric_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;

]


---


```r
Corn %&gt;% 
  ggplot(aes(x = Crossed,
             y = Self)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm") +
  theme_bw()
```

&lt;img src="statstudio_categorical_data_nonparametric_files/figure-html/unnamed-chunk-27-1.png" style="display: block; margin: auto;" /&gt;


---

Rememeber, essentially the paired t-test is focused on performing a one-sample *t*-test on the **difference** in scores between the paired data - testing whether the mean of the differences could potentially come from a population with `\(\mu = 0\)`
 
.pull-left[

```r
Corn_wide &lt;- Corn %&gt;% 
  dplyr::mutate(diff = Crossed - Self)

Corn_wide
```

```
   pair pot Crossed Self diff
1     1   1     188  130   58
2     2   1      96  163  -67
3     3   1     168  160    8
4     4   2     176  160   16
5     5   2     153  147    6
6     6   2     172  149   23
7     7   3     177  149   28
8     8   3     163  122   41
9     9   3     146  132   14
10   10   3     173  144   29
11   11   3     186  130   56
12   12   4     168  144   24
13   13   4     177  102   75
14   14   4     184  124   60
15   15   4      96  144  -48
```
]

--

.pull-right[


```r
Corn_wide$diff
```

```
 [1]  58 -67   8  16   6  23  28  41  14  29  56  24  75  60 -48
```




```r
mean(Corn_wide$diff)
```

```
[1] 21.53333
```
]

---


```r
set.seed(1)

shuffle1 &lt;- Corn_wide$diff * sample(c(-1,1), 
                                   15, 
                                   replace = TRUE)
shuffle1
```

```
 [1] -58 -67  -8 -16   6 -23 -28 -41  14  29 -56 -24 -75 -60  48
```



```r
mean(shuffle1)
```

```
[1] -23.93333
```

--


```r
shuffle2 &lt;- Corn_wide$diff * sample(c(-1,1), 
                                   15, 
                                   replace = TRUE)
shuffle2
```

```
 [1]  58 -67   8  16  -6 -23 -28 -41 -14 -29 -56  24 -75 -60 -48
```



```r
mean(shuffle2)
```

```
[1] -22.73333
```


---



```r
set.seed(2)

k &lt;- 10000

results &lt;- vector(length = k)

for(i in 1:k){
  results[i] &lt;-  mean(Corn_wide$diff * sample(c(-1,1), 
                                           length(Corn_wide$diff), 
                                           replace = TRUE))
    
}

means &lt;- data.frame(difs = unlist(results))
```


--


```r
summary(means)
```

```
      difs          
 Min.   :-36.06667  
 1st Qu.: -7.80000  
 Median :  0.20000  
 Mean   :  0.03016  
 3rd Qu.:  7.80000  
 Max.   : 33.93333  
```


---

.pull-leftsmall[


```r
means %&gt;% 
  ggplot(aes(x = difs)) +
  geom_histogram(color = "black",
                 alpha=.4, 
                 binwidth = 2) +
  geom_vline(color="red",
             lwd = 1,
             lty = 2,
             xintercept = mean(Corn_wide$diff)) +
  theme_bw()+
  labs(caption = paste("Note: ", N, "permutations"),
       x = "Difference = Crossed - Self Pollinated",
       y = "Frequency")
```


]

.pull-rightbig[
&lt;img src="statstudio_categorical_data_nonparametric_files/figure-html/unnamed-chunk-38-1.png" style="display: block; margin: auto;" /&gt;


P-value: one-sided


```r
sum(means &gt; mean(Corn_wide$diff))/N
```

```
[1] 0.0249
```

]

---

# Advantages

1. does not require residuals of sampled data to adhere to any speciﬁc (e.g., normal) distribution

2. is applicable when data violate many of the speciﬁc assumptions of other statistical techniques

3. test hypotheses that are based on the obtained data without reference to their relation to any speciﬁc population

4. are ﬂexible enough to analyze data from individual subjects/participants and groups of subjects/participants


---

#  Important Considerations 

1. computational intensiveness

2. how many permutations/samples should be drawn?

3. sample size calculations are vague

4. must assume exchangeable between the groups or treatments that are being compared


---

### Links

[Resampling Statistics: Randomization and the Bootstrap](https://www.uvm.edu/~statdhtx/StatPages/ResamplingWithR/ResamplingR.html)

The [`coin`](http://coin.r-forge.r-project.org/) package


[Data simulation and randomization tests](https://uoftcoders.github.io/rcourse/lec12-randomization-tests.html)
James S. Santangelo


[Chapter 16 Bootstrap Methods and Permutation Tests](http://bcs.whfreeman.com/webpub/statistics/ips9e/9781319013387/companionchapters/companionchapter16.pdf)

[Chapter 17 Shuffling labels to generate a null](https://bookdown.org/ybrandvain/Applied-Biostats/perm1.html)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
